import os
import json
import mmap
import time
import pathlib
import resource
import multiprocessing
from collections import Counter, defaultdict
import regex as re  # å¿…é¡»ä½¿ç”¨ regex åº“ä»¥æ”¯æŒ \p{L}


# ====================================================
# 1. ç‹¬ç«‹çš„ Worker å‡½æ•° (å®šä¹‰åœ¨ç±»å¤–éƒ¨ï¼Œé¿å… Pickle é—®é¢˜)
# ====================================================

def _worker_init(pattern_str, st_pattern_str):
    """
    å­è¿›ç¨‹åˆå§‹åŒ–å‡½æ•°ï¼šåªåœ¨è¿›ç¨‹å¯åŠ¨æ—¶è¿è¡Œä¸€æ¬¡ã€‚
    ç”¨äºç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼ï¼Œé¿å…æ¯æ¬¡ä»»åŠ¡éƒ½ä¼ é€’ç¼–è¯‘å¥½çš„å¯¹è±¡ã€‚
    """
    global shared_regex, shared_st_pattern
    shared_regex = re.compile(pattern_str)
    if st_pattern_str:
        shared_st_pattern = re.compile(st_pattern_str)
    else:
        shared_st_pattern = None

def _process_chunk_task(args):
    """
    å­è¿›ç¨‹çš„å·¥ä½œé€»è¾‘ï¼šè¯»å–æ–‡ä»¶å— -> é¢„åˆ†è¯ -> ç»Ÿè®¡é¢‘ç‡
    """
    file_path, start_byte, end_byte, special_tokens = args
    local_counts = Counter()

    try:
        with open(file_path, "rb") as f:
            f.seek(start_byte)
            # åªè¯»å–åˆ†é…ç»™å½“å‰è¿›ç¨‹çš„å­—èŠ‚å—
            bytes_data = f.read(end_byte - start_byte)

        # è§£ç  (errors='replace' é˜²æ­¢åˆ‡åˆ†ç‚¹ç¨å¾®åˆ‡åå­—èŠ‚ï¼Œè™½ç„¶åŸºäº endoftext åˆ‡åˆ†é€šå¸¸å®‰å…¨)
        text_chunk = bytes_data.decode("utf-8", errors="replace")

        # 1. åˆ‡åˆ† Special Tokens (ä¿æŠ¤ç‰¹æ®Šæ ‡è®°ä¸è¢«åç»­æ­£åˆ™æ‰“ç¢)
        if shared_st_pattern:
            parts = shared_st_pattern.split(text_chunk)
        else:
            parts = [text_chunk]

        for part in parts:
            # è·³è¿‡ç‰¹æ®Šæ ‡è®°æˆ–ç©ºå­—ç¬¦ä¸²
            if part in special_tokens or not part:
                continue

            # 2. GPT-2 æ­£åˆ™åˆ†è¯
            tokens = shared_regex.findall(part)
            
            # 3. ç»Ÿè®¡ (è½¬ä¸º bytes tuple)
            for token in tokens:
                token_bytes = token.encode("utf-8")
                # å°† b'abc' è½¬ä¸º (97, 98, 99)
                ids = tuple(bytes([b]) for b in token_bytes)
                local_counts[ids] += 1

    except Exception as e:
        # ç”Ÿäº§ç¯å¢ƒä¸­å»ºè®®ä½¿ç”¨ logging
        print(f"Worker Error processing {start_byte}-{end_byte}: {e}")
        return Counter()

    return local_counts


# ====================================================
# 2. ä¸»ç±»å®šä¹‰
# ====================================================

class BpeTrainMultiprocess:
    def __init__(self, 
                 input_path: str | os.PathLike,
                 vocab_size: int,
                 special_tokens: list[str],
                 chunk_size: int = 1024 * 1024 * 16, # 16MB per chunk
                 **kwargs):
        self.input_path = input_path
        self.vocab_size = vocab_size
        self.special_tokens = special_tokens
        self.chunk_size = chunk_size
        
        # åˆå§‹åŒ–è¯è¡¨
        self.vocab_init()
        
        # å‡†å¤‡æ­£åˆ™å­—ç¬¦ä¸² (æ³¨æ„ï¼šä¸åœ¨è¿™é‡Œç¼–è¯‘ï¼Œè€Œæ˜¯ä¼ å­—ç¬¦ä¸²ç»™å­è¿›ç¨‹)
        self.regex_pattern_str = r"""'(?:[sdmt]|ll|ve|re)| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"""
        
        self.sorted_special_tokens = sorted(self.special_tokens, key=len, reverse=True)
        if self.sorted_special_tokens:
            self.st_pattern_str = "(" + "|".join(re.escape(t) for t in self.sorted_special_tokens) + ")"
        else:
            self.st_pattern_str = None
            
        self.merges = []

    def vocab_init(self):
        """åˆå§‹åŒ–å­—å…¸: 0-255 bytes + special tokens"""
        self.vocab = {i: bytes([i]) for i in range(256)}
        # å»ºç«‹å€’æ’è¡¨ï¼šb'a' -> 97 (ç”¨äºå¿«é€Ÿå°† bytes è½¬ä¸º ID)
        self.vocab_inv = {bytes([i]): i for i in range(256)}

        for st in self.special_tokens:
            st_bytes = st.encode("utf-8")
            if st_bytes not in self.vocab_inv:
                new_id = len(self.vocab)
                self.vocab[new_id] = st_bytes
                self.vocab_inv[st_bytes] = new_id

    def get_chunk_boundaries(self) -> list[tuple[int, int]]:
        """åˆ©ç”¨ mmap å¿«é€Ÿå¯»æ‰¾åŸºäº special_token çš„åˆ‡åˆ†è¾¹ç•Œ"""
        boundaries = []
        if not os.path.exists(self.input_path):
            raise FileNotFoundError(f"Input file not found: {self.input_path}")
            
        file_size = os.path.getsize(self.input_path)
        
        # æå‰ encodeï¼Œé¿å…å¾ªç¯å†…é‡å¤ encode
        special_token_bytes = [st.encode("utf-8") for st in self.special_tokens]

        with open(self.input_path, "rb") as f:
            with mmap.mmap(f.fileno(), length=0, access=mmap.ACCESS_READ) as mm:
                start = 0
                while start < file_size:
                    target_end = min(start + self.chunk_size, file_size)
                    
                    if target_end == file_size:
                        boundaries.append((start, file_size))
                        break

                    # å¯»æ‰¾ chunk_size ä¹‹åæœ€è¿‘çš„ç‰¹æ®Šæ ‡è®°
                    next_delim_pos = -1
                    min_pos = float('inf')
                    
                    for st_bytes in special_token_bytes:
                        # ä» target_end å¼€å§‹æ‰¾
                        pos = mm.find(st_bytes, target_end)
                        if pos != -1 and pos < min_pos:
                            min_pos = pos
                    
                    if min_pos == float('inf'):
                        # åé¢æ²¡æœ‰ç‰¹æ®Šæ ‡è®°äº†ï¼Œè¿™ä¸€å—ç›´æ¥åˆ°æ–‡ä»¶æœ«å°¾
                        boundaries.append((start, file_size))
                        break
                    
                    # æˆ‘ä»¬é€‰æ‹©åœ¨ç‰¹æ®Šæ ‡è®°çš„ *èµ·å§‹ä½ç½®* åˆ‡æ–­
                    # Chunk 1: ... text ends here.
                    # Chunk 2: <|endoftext|> Next doc starts...
                    end = min_pos
                    boundaries.append((start, end))
                    start = end
        return boundaries

    def run_parallel_tokenization(self):
        """Step 1: å¹¶è¡Œè¯»å–ä¸é¢„åˆ†è¯"""
        print(f"[1/3] Calculating chunk boundaries for {self.input_path}...")
        boundaries = self.get_chunk_boundaries()
        print(f"      Split into {len(boundaries)} chunks.")

        # å‡†å¤‡ä»»åŠ¡å‚æ•°: (path, start, end, special_tokens_set)
        # ä¼ é€’ set æŸ¥æ‰¾æ›´å¿«
        st_set = set(self.special_tokens)
        tasks = [(self.input_path, start, end, st_set) for start, end in boundaries]

        print(f"[2/3] Parallel processing with {multiprocessing.cpu_count()} cores...")
        train_data_bytes = Counter()
        
        # ä½¿ç”¨ initializer åˆå§‹åŒ–å­è¿›ç¨‹çš„æ­£åˆ™
        with multiprocessing.Pool(
            processes=multiprocessing.cpu_count(),
            initializer=_worker_init,
            initargs=(self.regex_pattern_str, self.st_pattern_str)
        ) as pool:
            # imap_unordered æ€§èƒ½æ›´å¥½ï¼Œç»“æœé¡ºåºä¸é‡è¦
            for i, local_counts in enumerate(pool.imap_unordered(_process_chunk_task, tasks)):
                train_data_bytes.update(local_counts)
                if (i + 1) % 10 == 0:
                    print(f"      Processed {i + 1}/{len(boundaries)} chunks...")

        print(f"      Pre-tokenization complete. Unique byte-tuples: {len(train_data_bytes)}")
        return train_data_bytes

    def train(self):
        """æ‰§è¡Œå®Œæ•´çš„ BPE è®­ç»ƒæµç¨‹"""
        
        # 1. è·å–é¢„åˆ†è¯åçš„ byte tuples ç»Ÿè®¡
        train_data_bytes = self.run_parallel_tokenization()

        # 2. è½¬æ¢æ•°æ®æ ¼å¼: Bytes Tuple -> Int ID Tuple
        # æ•´æ•°è¿ç®—æ¯”å­—èŠ‚å¯¹è±¡è¿ç®—å¿«ï¼Œä¸”å…¼å®¹ vocab ç´¢å¼•
        print("[2.5/3] Converting bytes to IDs...")
        train_data = Counter()
        for byte_tuple, count in train_data_bytes.items():
            try:
                # åˆ©ç”¨åˆå§‹åŒ–å¥½çš„ vocab_inv å¿«é€ŸæŸ¥æ‰¾
                id_tuple = tuple(self.vocab_inv[b] for b in byte_tuple)
                train_data[id_tuple] = count
            except KeyError:
                # ç†è®ºä¸Šä¸ä¼šå‘ç”Ÿï¼Œé™¤éæ­£åˆ™åˆ‡åˆ†å‡ºäº† 0-255 ä»¥å¤–çš„å­—èŠ‚ï¼ˆä¸å¯èƒ½ï¼‰
                pass
        
        # é‡Šæ”¾æ—§å†…å­˜
        del train_data_bytes

        # 3. BPE å¾ªç¯ (ä½¿ç”¨å€’æ’ç´¢å¼•ä¼˜åŒ–)
        print("[3/3] Starting Fast BPE Loop (Inverted Index)...")
        
        # --- æ„å»ºå€’æ’ç´¢å¼• ---
        # stats: è®°å½• pair çš„é¢‘ç‡ {(id1, id2): count}
        stats = Counter()
        # indices: å€’æ’ç´¢å¼• {pair: {word_tuple, ...}}
        # è®°å½•æ¯ä¸ª pair å‡ºç°åœ¨äº†å“ªäº›å•è¯ä¸­ï¼Œå‡å°‘åç»­éå†èŒƒå›´
        indices = defaultdict(set)
        
        for ids, count in train_data.items():
            for i in range(len(ids) - 1):
                pair = (ids[i], ids[i+1])
                stats[pair] += count
                indices[pair].add(ids)

        # --- å¾ªç¯åˆå¹¶ ---
        while len(self.vocab) < self.vocab_size:
            if not stats:
                break

            # Tie-breaking: å…ˆæ¯”é¢‘ç‡(é«˜ä¼˜)ï¼Œå†æ¯”å­—èŠ‚åº(å­—å…¸åºå¤§ä¼˜)
            # æ³¨æ„ï¼šå¿…é¡»å» vocab é‡ŒæŸ¥ bytes å†…å®¹æ¥æ¯”è¾ƒï¼Œä¸èƒ½ç›´æ¥æ¯” ID
            best_pair = max(stats, key=lambda p: (stats[p], self.vocab[p[0]], self.vocab[p[1]]))
            
            # å¦‚æœæœ€ä½³ pair çš„é¢‘ç‡å·²ç»å½’é›¶ï¼ˆå¯èƒ½è¢«å…¶ä»–åˆå¹¶ç ´åäº†ï¼‰ï¼Œè·³å‡º
            if stats[best_pair] == 0:
                break

            # æ‰§è¡Œ Merge
            new_id = len(self.vocab)
            part1 = self.vocab[best_pair[0]]
            part2 = self.vocab[best_pair[1]]
            self.vocab[new_id] = part1 + part2
            self.merges.append((part1, part2))

            # æ‰“å°è¿›åº¦
            if len(self.vocab) % 100 == 0:
                print(f"      Vocab size: {len(self.vocab)}/{self.vocab_size} | Merged: {best_pair} -> {new_id}")

            # --- å¿«é€Ÿæ›´æ–°é€»è¾‘ (åªæ›´æ–°ç›¸å…³å•è¯) ---
            # è·å–æ‰€æœ‰åŒ…å« best_pair çš„å•è¯åˆ—è¡¨
            words_to_update = list(indices[best_pair])
            changes = [] # æš‚å­˜å˜æ›´ï¼š(old_ids, new_ids, count)

            for old_ids in words_to_update:
                count = train_data[old_ids]
                new_ids_list = []
                i = 0
                
                # åœ¨å½“å‰å•è¯ä¸­æ‰§è¡Œæ›¿æ¢
                while i < len(old_ids):
                    if i < len(old_ids) - 1 and old_ids[i] == best_pair[0] and old_ids[i+1] == best_pair[1]:
                        # æ‰¾åˆ°åŒ¹é…ï¼æ‰§è¡Œåˆå¹¶
                        
                        # A. ç»´æŠ¤å·¦é‚»å±…çš„ç»Ÿè®¡
                        if new_ids_list:
                            prev = new_ids_list[-1]
                            # æ—§é‚»å±… (prev, best_pair[0]) é¢‘ç‡å‡å°‘
                            old_prev_pair = (prev, old_ids[i])
                            stats[old_prev_pair] -= count
                            if stats[old_prev_pair] == 0: del stats[old_prev_pair]
                            if old_prev_pair in indices: indices[old_prev_pair].discard(old_ids)
                            
                            # æ–°é‚»å±… (prev, new_id) é¢‘ç‡å¢åŠ 
                            new_prev_pair = (prev, new_id)
                            stats[new_prev_pair] += count
                            indices[new_prev_pair].add(old_ids) # æ³¨æ„ï¼šè¿™é‡Œæš‚æ—¶è¿˜å­˜çš„æ˜¯ old_ids, ç¨åæ‰¹é‡æ¸…ç†

                        # B. ç»´æŠ¤å³é‚»å±…çš„ç»Ÿè®¡ (å¦‚æœåˆå¹¶æ‰“æ–­äº†å³è¾¹çš„ pair)
                        # æ¯”å¦‚ A B Cï¼Œåˆå¹¶ A B -> ABã€‚åŸæ¥çš„ (B, C) å°±ä¸å­˜åœ¨äº†
                        if i + 2 < len(old_ids):
                            old_next_pair = (old_ids[i+1], old_ids[i+2])
                            stats[old_next_pair] -= count
                            if stats[old_next_pair] == 0: del stats[old_next_pair]
                            if old_next_pair in indices: indices[old_next_pair].discard(old_ids)

                        new_ids_list.append(new_id)
                        i += 2 # è·³è¿‡ä¸¤ä¸ªå…ƒç´ 
                    else:
                        new_ids_list.append(old_ids[i])
                        i += 1
                
                new_ids_tuple = tuple(new_ids_list)
                changes.append((old_ids, new_ids_tuple, count))
            
            # --- æ‰¹é‡åº”ç”¨ Train Data å’Œ Indices çš„å˜æ›´ ---
            # 1. å½»åº•åˆ é™¤ best_pair çš„è®°å½•
            del indices[best_pair]
            del stats[best_pair]

            for old_ids, new_ids, count in changes:
                # 2. ä» train_data ç§»é™¤æ—§å•è¯ï¼Œæ·»åŠ æ–°å•è¯
                if old_ids in train_data:
                    del train_data[old_ids]
                train_data[new_ids] += count
                
                # 3. ä¿®æ­£ Indices æŒ‡å‘
                # ä¸Šé¢çš„å¾ªç¯ä¸­ï¼Œæˆ‘ä»¬ä»…ä»…æ˜¯ä» indices ä¸­ discard äº† old_ids
                # æˆ‘ä»¬éœ€è¦æŠŠ new_ids åŠ å…¥åˆ°å®ƒåŒ…å«çš„æ‰€æœ‰ pair çš„ç´¢å¼•ä¸­
                # ä¼˜åŒ–ï¼šå…¶å®åªéœ€è¦æ›´æ–°ä¸ new_id ç›¸å…³çš„ pair å³å¯ï¼Œä½†å…¨é‡æ›´æ–°æ›´ä¸å®¹æ˜“å‡ºé”™
                # å®é™…ä¸Šï¼Œåœ¨ä¸Šé¢çš„ A æ­¥éª¤ä¸­ï¼Œæˆ‘ä»¬å‘ indices åŠ äº† old_ids (ä½œä¸ºå ä½)ã€‚
                # è¿™é‡Œçš„é€»è¾‘ç¨å¾®å¤æ‚ï¼Œä¸ºäº†ä»£ç æ¸…æ™°ï¼Œæˆ‘ä»¬é‡‡ç”¨ç®€å•çš„â€œé‡æ–°æ³¨å†Œæ–°è¯â€ç­–ç•¥ï¼š
                
                # å°†æ–°å•è¯åŠ å…¥åˆ°å®ƒåŒ…å«çš„æ‰€æœ‰ pair çš„ç´¢å¼•ä¸­
                for i in range(len(new_ids) - 1):
                    p = (new_ids[i], new_ids[i+1])
                    indices[p].add(new_ids)
                    # åŒæ—¶è¦æ¸…ç†æ‰æ—§çš„ old_ids (å¦‚æœä¹‹å‰æ²¡æ¸…å¹²å‡€)
                    if old_ids in indices[p]:
                        indices[p].discard(old_ids)

        return self.vocab, self.merges

# ====================================================
# 3. è¾…åŠ©æŠ¥å‘Šå‡½æ•°
# ====================================================

def save_and_report(vocab, merges, elapsed_time, output_dir="output"):
    os.makedirs(output_dir, exist_ok=True)
    
    # è·å–å†…å­˜å³°å€¼ (å…¼å®¹ Linux/macOS)
    try:
        peak_memory_kb = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
        # Linux: KB, MacOS: Bytes (éœ€è¦åˆ¤æ–­ç³»ç»Ÿï¼Œè¿™é‡Œç®€å•æŒ‰ KB å¤„ç†ï¼Œé€šå¸¸æœåŠ¡å™¨æ˜¯ Linux)
        import platform
        if platform.system() == 'Darwin': # MacOS
             peak_memory_mb = peak_memory_kb / (1024 * 1024)
        else:
             peak_memory_mb = peak_memory_kb / 1024
    except:
        peak_memory_mb = 0.0

    print(f"\nğŸ“Š Performance Report:")
    print(f"----------------------")
    print(f"Time Taken  : {elapsed_time:.2f} seconds ({elapsed_time/3600:.4f} hours)")
    print(f"Peak Memory : {peak_memory_mb:.2f} MB")

    # ç»Ÿè®¡æœ€é•¿ Token
    if vocab:
        longest_token_bytes = max(vocab.values(), key=len)
        print(f"Longest Token Length: {len(longest_token_bytes)} bytes")
        try:
            print(f"Longest Token Content: {longest_token_bytes.decode('utf-8')}")
        except UnicodeDecodeError:
            print(f"Longest Token Content (repr): {repr(longest_token_bytes)}")

    print(f"\nğŸ’¾ Saving to {output_dir}...")

    # ä¿å­˜ Vocab
    json_vocab = {}
    for token_id, token_bytes in vocab.items():
        try:
            token_str = token_bytes.decode("utf-8")
        except UnicodeDecodeError:
            token_str = str(token_bytes)
        json_vocab[token_id] = token_str

    with open(os.path.join(output_dir, "vocab.json"), "w", encoding="utf-8") as f:
        json.dump(json_vocab, f, indent=2, ensure_ascii=False)

    # ä¿å­˜ Merges
    with open(os.path.join(output_dir, "merges.txt"), "w", encoding="utf-8") as f:
        f.write("#version: 0.2\n") 
        for p1, p2 in merges:
            # å°† bytes è§£ç å¹¶ç”¨ Ä  æ›¿æ¢ç©ºæ ¼ï¼Œæ–¹ä¾¿å¯è§†åŒ–
            s1 = p1.decode("utf-8", errors="replace").replace(" ", "Ä ")
            s2 = p2.decode("utf-8", errors="replace").replace(" ", "Ä ")
            f.write(f"{s1} {s2}\n")

    print("Done.")

# ====================================================
# 4. æ‰§è¡Œå…¥å£
# ====================================================

if __name__ == "__main__":
    # é…ç½®åŒº
    VOCAB_SIZE = 1000  # æµ‹è¯•ç”¨ï¼Œå®é™…å¯è®¾ä¸º 32000 æˆ–æ›´å¤§
    SPECIAL_TOKENS = ["<|endoftext|>"]
    
    # è·¯å¾„é…ç½®
    project_path = pathlib.Path(__file__).resolve().parent.parent.parent
    # å‡è®¾ä½ çš„æ–‡ä»¶åœ¨è¿™ä¸ªä½ç½®ï¼Œå¦‚æœä¸å­˜åœ¨ï¼Œè¯·ä¿®æ”¹è·¯å¾„
    input_path = os.path.join(project_path, "data/TinyStoriesV2-GPT4-train.txt")
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(input_path):
        print(f"âŒ Error: Input file not found at {input_path}")
        # ä¸ºäº†æ¼”ç¤ºï¼Œå¯ä»¥ç”Ÿæˆä¸€ä¸ªå‡çš„æµ‹è¯•æ–‡ä»¶
        # print("Creating dummy file...")
        # with open("dummy_corpus.txt", "w") as f: f.write("Hello world " * 10000)
        # input_path = "dummy_corpus.txt"
    else:
        print(f"ğŸš€ Starting BPE Training on {input_path}")
        print(f"   Target Vocab Size: {VOCAB_SIZE}")
        
        start_time = time.time()
        
        bpe = BpeTrainMultiprocess(
            input_path=input_path,
            vocab_size=VOCAB_SIZE,
            special_tokens=SPECIAL_TOKENS
        )
        
        # å¼€å§‹è®­ç»ƒ
        bpe.train()

        end_time = time.time()
        
        save_and_report(
            vocab=bpe.vocab,
            merges=bpe.merges,
            elapsed_time=end_time - start_time
        )